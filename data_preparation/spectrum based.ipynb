{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5e409cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import torch\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import cv2\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "DATA_DIR = r\"D:\\Documents\\datasets\\AIST4010\\muse\"\n",
    "SPEC_DIR = os.path.join(DATA_DIR, \"spectrograms_jpg\")\n",
    "songs_data_fp = os.path.join(DATA_DIR, \"extracted_data.csv\")\n",
    "\n",
    "\n",
    "def load_imgs(fp=SPEC_DIR, transform=None):\n",
    "    fp = glob.glob(os.path.join(fp, '*'))\n",
    "    rematch_pattern = r\"^.*\\\\([^\\.]*).jpg\"\n",
    "    fp.sort(key=lambda fp: re.match(rematch_pattern, fp).group(1))\n",
    "    imgs = [None] * len(fp)\n",
    "    img_ids = [None] * len(fp)\n",
    "    transform = transforms.ToTensor()\n",
    "    for idx, img_fp in enumerate(fp):\n",
    "        img_id = re.match(rematch_pattern, img_fp).group(1)\n",
    "        with Image.open(img_fp) as f:\n",
    "            imgs[idx] = np.asarray(f.convert(\"RGB\"))\n",
    "        img_ids[idx] = img_id\n",
    "    return np.array(imgs), np.array(img_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c8273571",
   "metadata": {},
   "outputs": [],
   "source": [
    "songs_data = pd.read_csv(songs_data_fp)\n",
    "songs_data.set_index(\"spotify_id\", inplace=True)\n",
    "\n",
    "data, ids = load_imgs()\n",
    "labels = songs_data.loc[ids, [\"valence_tags\", \"arousal_tags\", \"dominance_tags\"]].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2feed9db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, data, labels, transform=None):\n",
    "        super(ImageDataset, self).__init__()\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.to_list()\n",
    "        sample = self.data[idx]\n",
    "        if self.transform:\n",
    "            sample = self.transform(self.data[idx])\n",
    "        return sample, self.labels[idx]\n",
    "\n",
    "    def get_data(self, idx=None):\n",
    "        if idx:\n",
    "            return self.data[idx]\n",
    "        return self.data\n",
    "\n",
    "    \n",
    "def spectrum_transform():\n",
    "    trans = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    return trans\n",
    "\n",
    "\n",
    "spectrum_ds = ImageDataset(data, labels, transform=spectrum_transform())\n",
    "spectrum_loader = DataLoader(spectrum_ds, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1f9aaadd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PlainCNN(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim):\n",
    "        super(PlainCNN, self).__init__()\n",
    "        conv_stack = [\n",
    "            nn.Conv2d(in_dim, 32, 3),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(32, 64, 3),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, 3),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(64, 128, 3),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 128, 3),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Dropout2d(0.5),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(73728, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, out_dim)\n",
    "        ]\n",
    "        self.conv_stack = nn.Sequential(*conv_stack)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.conv_stack(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1219489c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss - 2.03711\t1.18359\t1.55664\n",
      "train loss - 0.87451\t0.49316\t0.52881\n",
      "train loss - 0.87109\t0.48828\t0.52344\n",
      "train loss - 0.84424\t0.47119\t0.50146\n",
      "train loss - 0.84326\t0.47314\t0.49878\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [9]\u001b[0m, in \u001b[0;36m<cell line: 11>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(EPOCHS):\n\u001b[0;32m     12\u001b[0m     running_val_loss, running_aro_loss, running_dom_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m, \u001b[38;5;241m0.0\u001b[39m, \u001b[38;5;241m0.0\u001b[39m\n\u001b[1;32m---> 13\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m inputs, labels \u001b[38;5;129;01min\u001b[39;00m spectrum_loader:\n\u001b[0;32m     14\u001b[0m         optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m     16\u001b[0m         inputs, labels \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mhalf()\u001b[38;5;241m.\u001b[39mto(device), labels\u001b[38;5;241m.\u001b[39mhalf()\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[1;32mD:\\venv\\python\\music-analysis\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:521\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    519\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    520\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()\n\u001b[1;32m--> 521\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    522\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    523\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    524\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    525\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mD:\\venv\\python\\music-analysis\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:561\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    559\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    560\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 561\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    562\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    563\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data)\n",
      "File \u001b[1;32mD:\\venv\\python\\music-analysis\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:49\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfetch\u001b[39m(\u001b[38;5;28mself\u001b[39m, possibly_batched_index):\n\u001b[0;32m     48\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[1;32m---> 49\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     51\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mD:\\venv\\python\\music-analysis\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:49\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfetch\u001b[39m(\u001b[38;5;28mself\u001b[39m, possibly_batched_index):\n\u001b[0;32m     48\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[1;32m---> 49\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     51\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Input \u001b[1;32mIn [3]\u001b[0m, in \u001b[0;36mImageDataset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     17\u001b[0m sample \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata[idx]\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform:\n\u001b[1;32m---> 19\u001b[0m     sample \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m sample, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabels[idx]\n",
      "File \u001b[1;32mD:\\venv\\python\\music-analysis\\lib\\site-packages\\torchvision\\transforms\\transforms.py:61\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[0;32m     60\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms:\n\u001b[1;32m---> 61\u001b[0m         img \u001b[38;5;241m=\u001b[39m \u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     62\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "File \u001b[1;32mD:\\venv\\python\\music-analysis\\lib\\site-packages\\torchvision\\transforms\\transforms.py:98\u001b[0m, in \u001b[0;36mToTensor.__call__\u001b[1;34m(self, pic)\u001b[0m\n\u001b[0;32m     90\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, pic):\n\u001b[0;32m     91\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     92\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m     93\u001b[0m \u001b[38;5;124;03m        pic (PIL Image or numpy.ndarray): Image to be converted to tensor.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;124;03m        Tensor: Converted image.\u001b[39;00m\n\u001b[0;32m     97\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 98\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpic\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\venv\\python\\music-analysis\\lib\\site-packages\\torchvision\\transforms\\functional.py:126\u001b[0m, in \u001b[0;36mto_tensor\u001b[1;34m(pic)\u001b[0m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pic\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[0;32m    124\u001b[0m     pic \u001b[38;5;241m=\u001b[39m pic[:, :, \u001b[38;5;28;01mNone\u001b[39;00m]\n\u001b[1;32m--> 126\u001b[0m img \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_numpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpic\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtranspose\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontiguous\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    127\u001b[0m \u001b[38;5;66;03m# backward compatibility\u001b[39;00m\n\u001b[0;32m    128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(img, torch\u001b[38;5;241m.\u001b[39mByteTensor):\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = PlainCNN(3, 3).half().to(device)\n",
    "\n",
    "\n",
    "# training settings\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9, weight_decay=0.01)\n",
    "\n",
    "\n",
    "EPOCHS = 100\n",
    "for epoch in range(EPOCHS):\n",
    "    running_val_loss, running_aro_loss, running_dom_loss = 0.0, 0.0, 0.0\n",
    "    for inputs, labels in spectrum_loader:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        inputs, labels = inputs.half().to(device), labels.half().to(device)\n",
    "        outputs = model(inputs)\n",
    "        val_loss = criterion(outputs[:, 0], labels[:, 0]) / 3\n",
    "        aro_loss = criterion(outputs[:, 1], labels[:, 1]) / 3\n",
    "        dom_loss = criterion(outputs[:, 2], labels[:, 2]) / 3\n",
    "        running_val_loss += val_loss * len(inputs)\n",
    "        running_aro_loss += aro_loss * len(inputs)\n",
    "        running_dom_loss += dom_loss * len(inputs)\n",
    "        \n",
    "        val_loss.backward(retain_graph=True)\n",
    "        aro_loss.backward(retain_graph=True)\n",
    "        dom_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    epoch_val_loss = running_val_loss / len(spectrum_loader.dataset)\n",
    "    epoch_aro_loss = running_aro_loss / len(spectrum_loader.dataset)\n",
    "    epoch_dom_loss = running_dom_loss / len(spectrum_loader.dataset)\n",
    "    print(f\"train loss - {epoch_val_loss:.5f}\\t{epoch_aro_loss:.5f}\\t{epoch_dom_loss:.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e061e433",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[4.6133, 3.5527, 4.4336],\n",
      "        [6.1016, 4.1133, 5.6094],\n",
      "        [8.2500, 5.4727, 7.2656],\n",
      "        [4.6680, 4.3242, 4.5234],\n",
      "        [4.4258, 3.9531, 4.3789],\n",
      "        [5.2656, 3.4160, 5.0664],\n",
      "        [6.9141, 4.1992, 6.1992],\n",
      "        [3.1992, 3.2480, 3.3398],\n",
      "        [3.6309, 3.2793, 3.7227],\n",
      "        [6.2109, 4.6016, 5.7734],\n",
      "        [4.0625, 3.2188, 4.0625],\n",
      "        [5.6445, 4.1719, 5.2266],\n",
      "        [4.8516, 3.0137, 4.5703],\n",
      "        [6.4883, 4.6133, 5.9805],\n",
      "        [6.6484, 4.7500, 6.0508],\n",
      "        [6.2266, 5.3203, 5.9023],\n",
      "        [6.1680, 3.9980, 5.6758],\n",
      "        [4.1680, 3.9082, 4.0859],\n",
      "        [3.8516, 3.5762, 3.8691],\n",
      "        [5.8750, 4.6523, 5.4922],\n",
      "        [6.1914, 3.9746, 5.7461],\n",
      "        [4.7734, 4.2578, 4.7031],\n",
      "        [5.2773, 4.6289, 5.0664],\n",
      "        [5.5430, 4.4141, 5.1836],\n",
      "        [5.2266, 3.6758, 5.0664],\n",
      "        [6.2539, 4.6211, 5.8477],\n",
      "        [5.7500, 5.0625, 5.4336],\n",
      "        [6.6328, 5.2383, 6.1250],\n",
      "        [7.1367, 4.7031, 6.4336],\n",
      "        [4.9492, 3.9648, 4.7930],\n",
      "        [6.3633, 4.7109, 5.9141],\n",
      "        [5.9688, 3.9434, 5.5352]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor([[3.7598, 2.4492, 3.1406],\n",
      "        [7.4492, 4.1914, 7.0000],\n",
      "        [8.3594, 6.1562, 6.9688],\n",
      "        [3.9395, 4.3594, 4.8281],\n",
      "        [2.3848, 3.9297, 3.5859],\n",
      "        [4.7695, 4.3398, 4.7031],\n",
      "        [6.6523, 4.7422, 6.0859],\n",
      "        [1.8701, 2.0645, 1.9805],\n",
      "        [6.6484, 4.7695, 5.3086],\n",
      "        [7.0039, 4.5547, 6.5859],\n",
      "        [2.5137, 3.3242, 3.4336],\n",
      "        [5.8906, 4.2539, 5.1055],\n",
      "        [5.0898, 4.2109, 4.5195],\n",
      "        [6.6797, 4.3711, 5.0508],\n",
      "        [5.6484, 5.3711, 5.2266],\n",
      "        [5.0000, 5.2812, 6.2383],\n",
      "        [4.3203, 3.5508, 5.1719],\n",
      "        [4.1914, 4.5508, 3.6309],\n",
      "        [4.0000, 2.6797, 2.9609],\n",
      "        [5.3867, 3.4688, 4.7188],\n",
      "        [5.5898, 3.1465, 5.1172],\n",
      "        [3.8906, 5.7227, 4.7305],\n",
      "        [3.4336, 5.3438, 4.9531],\n",
      "        [4.2383, 5.1914, 5.3516],\n",
      "        [5.4609, 3.6582, 5.0820],\n",
      "        [6.4609, 5.4102, 6.9492],\n",
      "        [6.5195, 4.6250, 4.9766],\n",
      "        [5.8320, 4.9414, 5.2148],\n",
      "        [5.1758, 5.9766, 5.6289],\n",
      "        [4.5859, 4.1445, 4.4883],\n",
      "        [7.3828, 5.1445, 6.1914],\n",
      "        [5.7852, 3.1953, 5.4844]], device='cuda:0', dtype=torch.float16)\n"
     ]
    }
   ],
   "source": [
    "for x, y in spectrum_loader:\n",
    "    x, y = x.half().to(device), y.half().to(device)\n",
    "    outputs = model(x)\n",
    "    print(outputs)\n",
    "    print(y)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca6a42da",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "music-analysis",
   "language": "python",
   "name": "music-analysis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
