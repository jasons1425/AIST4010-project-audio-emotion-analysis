{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "acd9ad85",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchlibrosa.stft import Spectrogram, LogmelFilterBank\n",
    "from torchlibrosa.augmentation import SpecAugmentation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import os\n",
    "import glob\n",
    "import re\n",
    "\n",
    "test_data_path = r\"D:\\Documents\\datasets\\AIST4010\\muse\\wavs\\000xQL6tZNLJzIrtIgxqSl.npy\"\n",
    "checkpoint_path = r\"Wavegram_Cnn14_mAP=0.389.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3af06bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_layer(layer):\n",
    "    \"\"\"Initialize a Linear or Convolutional layer. \"\"\"\n",
    "    nn.init.xavier_uniform_(layer.weight)\n",
    " \n",
    "    if hasattr(layer, 'bias'):\n",
    "        if layer.bias is not None:\n",
    "            layer.bias.data.fill_(0.)\n",
    "            \n",
    "    \n",
    "def init_bn(bn):\n",
    "    \"\"\"Initialize a Batchnorm layer. \"\"\"\n",
    "    bn.bias.data.fill_(0.)\n",
    "    bn.weight.data.fill_(1.)\n",
    "\n",
    "\n",
    "class ConvPreWavBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        \n",
    "        super(ConvPreWavBlock, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv1d(in_channels=in_channels, \n",
    "                              out_channels=out_channels,\n",
    "                              kernel_size=3, stride=1,\n",
    "                              padding=1, bias=False)\n",
    "                              \n",
    "        self.conv2 = nn.Conv1d(in_channels=out_channels, \n",
    "                              out_channels=out_channels,\n",
    "                              kernel_size=3, stride=1, dilation=2, \n",
    "                              padding=2, bias=False)\n",
    "                              \n",
    "        self.bn1 = nn.BatchNorm1d(out_channels)\n",
    "        self.bn2 = nn.BatchNorm1d(out_channels)\n",
    "\n",
    "        self.init_weight()\n",
    "        \n",
    "    def init_weight(self):\n",
    "        init_layer(self.conv1)\n",
    "        init_layer(self.conv2)\n",
    "        init_bn(self.bn1)\n",
    "        init_bn(self.bn2)\n",
    "\n",
    "        \n",
    "    def forward(self, input, pool_size):\n",
    "        \n",
    "        x = input\n",
    "        x = F.relu_(self.bn1(self.conv1(x)))\n",
    "        x = F.relu_(self.bn2(self.conv2(x)))\n",
    "        x = F.max_pool1d(x, kernel_size=pool_size)\n",
    "        \n",
    "        return x\n",
    "\n",
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        \n",
    "        super(ConvBlock, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(in_channels=in_channels, \n",
    "                              out_channels=out_channels,\n",
    "                              kernel_size=(3, 3), stride=(1, 1),\n",
    "                              padding=(1, 1), bias=False)\n",
    "                              \n",
    "        self.conv2 = nn.Conv2d(in_channels=out_channels, \n",
    "                              out_channels=out_channels,\n",
    "                              kernel_size=(3, 3), stride=(1, 1),\n",
    "                              padding=(1, 1), bias=False)\n",
    "                              \n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "        self.init_weight()\n",
    "        \n",
    "    def init_weight(self):\n",
    "        init_layer(self.conv1)\n",
    "        init_layer(self.conv2)\n",
    "        init_bn(self.bn1)\n",
    "        init_bn(self.bn2)\n",
    "\n",
    "        \n",
    "    def forward(self, input, pool_size=(2, 2), pool_type='avg'):\n",
    "        \n",
    "        x = input\n",
    "        x = F.relu_(self.bn1(self.conv1(x)))\n",
    "        x = F.relu_(self.bn2(self.conv2(x)))\n",
    "        if pool_type == 'max':\n",
    "            x = F.max_pool2d(x, kernel_size=pool_size)\n",
    "        elif pool_type == 'avg':\n",
    "            x = F.avg_pool2d(x, kernel_size=pool_size)\n",
    "        elif pool_type == 'avg+max':\n",
    "            x1 = F.avg_pool2d(x, kernel_size=pool_size)\n",
    "            x2 = F.max_pool2d(x, kernel_size=pool_size)\n",
    "            x = x1 + x2\n",
    "        else:\n",
    "            raise Exception('Incorrect argument!')\n",
    "        \n",
    "        return x   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f76094ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Wavegram_Cnn14(nn.Module):\n",
    "    def __init__(self, sample_rate, window_size, hop_size, mel_bins, fmin, \n",
    "        fmax, classes_num):\n",
    "        \n",
    "        super(Wavegram_Cnn14, self).__init__()\n",
    "\n",
    "        window = 'hann'\n",
    "        center = True\n",
    "        pad_mode = 'reflect'\n",
    "        ref = 1.0\n",
    "        amin = 1e-10\n",
    "        top_db = None\n",
    "\n",
    "        self.pre_conv0 = nn.Conv1d(in_channels=1, out_channels=64, kernel_size=11, stride=5, padding=5, bias=False)\n",
    "        self.pre_bn0 = nn.BatchNorm1d(64)\n",
    "        self.pre_block1 = ConvPreWavBlock(64, 64)\n",
    "        self.pre_block2 = ConvPreWavBlock(64, 128)\n",
    "        self.pre_block3 = ConvPreWavBlock(128, 128)\n",
    "        self.pre_block4 = ConvBlock(in_channels=4, out_channels=64)\n",
    "\n",
    "        # Spec augmenter\n",
    "        self.spec_augmenter = SpecAugmentation(time_drop_width=64, time_stripes_num=2, \n",
    "            freq_drop_width=8, freq_stripes_num=2)\n",
    "\n",
    "        self.bn0 = nn.BatchNorm2d(64)\n",
    "\n",
    "        self.conv_block1 = ConvBlock(in_channels=1, out_channels=64)\n",
    "        self.conv_block2 = ConvBlock(in_channels=64, out_channels=128)\n",
    "        self.conv_block3 = ConvBlock(in_channels=128, out_channels=256)\n",
    "        self.conv_block4 = ConvBlock(in_channels=256, out_channels=512)\n",
    "        self.conv_block5 = ConvBlock(in_channels=512, out_channels=1024)\n",
    "        self.conv_block6 = ConvBlock(in_channels=1024, out_channels=2048)\n",
    "\n",
    "        self.fc1 = nn.Linear(2048, 2048, bias=True)\n",
    "        self.fc_audioset = nn.Linear(2048, classes_num, bias=True)\n",
    "        \n",
    "        self.init_weight()\n",
    "\n",
    "    def init_weight(self):\n",
    "        init_layer(self.pre_conv0)\n",
    "        init_bn(self.pre_bn0)\n",
    "        init_bn(self.bn0)\n",
    "        init_layer(self.fc1)\n",
    "        init_layer(self.fc_audioset)\n",
    " \n",
    "    def forward(self, input, mixup_lambda=None):\n",
    "        \"\"\"\n",
    "        Input: (batch_size, data_length)\"\"\"\n",
    "\n",
    "        # Wavegram\n",
    "        a1 = F.relu_(self.pre_bn0(self.pre_conv0(input[:, None, :])))\n",
    "        a1 = self.pre_block1(a1, pool_size=4)\n",
    "        a1 = self.pre_block2(a1, pool_size=4)\n",
    "        a1 = self.pre_block3(a1, pool_size=4)\n",
    "        a1 = a1.reshape((a1.shape[0], -1, 32, a1.shape[-1])).transpose(2, 3)\n",
    "        a1 = self.pre_block4(a1, pool_size=(2, 1))\n",
    "\n",
    "        # Mixup on spectrogram\n",
    "        if self.training and mixup_lambda is not None:\n",
    "            a1 = do_mixup(a1, mixup_lambda)\n",
    "        \n",
    "        x = a1\n",
    "        x = F.dropout(x, p=0.2, training=self.training)\n",
    "        x = self.conv_block2(x, pool_size=(2, 2), pool_type='avg')\n",
    "        x = F.dropout(x, p=0.2, training=self.training)\n",
    "        x = self.conv_block3(x, pool_size=(2, 2), pool_type='avg')\n",
    "        x = F.dropout(x, p=0.2, training=self.training)\n",
    "        x = self.conv_block4(x, pool_size=(2, 2), pool_type='avg')\n",
    "        x = F.dropout(x, p=0.2, training=self.training)\n",
    "        x = self.conv_block5(x, pool_size=(2, 2), pool_type='avg')\n",
    "        x = F.dropout(x, p=0.2, training=self.training)\n",
    "        x = self.conv_block6(x, pool_size=(1, 1), pool_type='avg')\n",
    "        x = F.dropout(x, p=0.2, training=self.training)\n",
    "        x = torch.mean(x, dim=3)\n",
    "        \n",
    "        (x1, _) = torch.max(x, dim=2)\n",
    "        x2 = torch.mean(x, dim=2)\n",
    "        x = x1 + x2\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = F.relu_(self.fc1(x))\n",
    "        embedding = F.dropout(x, p=0.5, training=self.training)\n",
    "        clipwise_output = torch.sigmoid(self.fc_audioset(x))\n",
    "        \n",
    "        output_dict = {'clipwise_output': clipwise_output, 'embedding': embedding}\n",
    "\n",
    "        return output_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a7bf0601",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WaveNet(nn.Module):\n",
    "    def __init__(self, out_dim, fe_dim=2048, \n",
    "                 sr=22050, wsize=520, hsize=320, mel_bins=128, fmin=50, fmax=8000, \n",
    "                 fcs=[], dropout=0.2, act=nn.ReLU, init=nn.init.kaiming_normal_):\n",
    "        super().__init__()\n",
    "        self.wavecnn = Wavegram_Cnn14(sample_rate=sr, window_size=wsize, \n",
    "                                      hop_size=hsize, mel_bins=mel_bins, \n",
    "                                      fmin=fmin, fmax=fmax, classes_num=527)\n",
    "        self.wavecnn.fc_audioset.require_grad = False\n",
    "        checkpoint = torch.load(r\"Wavegram_Cnn14_mAP=0.389.pth\", \n",
    "                                map_location=\"cuda\")\n",
    "        self.wavecnn.load_state_dict(checkpoint['model'])\n",
    "        fcs = [fe_dim] + fcs + [out_dim]\n",
    "        fc_layers = []\n",
    "        for idx in range(1, len(fcs)):\n",
    "            if idx != 1:\n",
    "                fc_layers.append(nn.Dropout(dropout))\n",
    "            fc_layers.append(nn.Linear(fcs[idx-1], fcs[idx]))\n",
    "            if act and idx != (len(fcs) - 1):\n",
    "                fc_layers.append(act())\n",
    "        if init:\n",
    "            for layer in fc_layers:\n",
    "                if type(layer) == nn.Linear:\n",
    "                    init(layer.weight)\n",
    "        self.classifier = nn.Sequential(*fc_layers)\n",
    "\n",
    "    def forward(self, x, mixup_lambda=None):\n",
    "        spectrum_features = self.wavecnn(x, mixup_lambda)['embedding']\n",
    "        return self.classifier(spectrum_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2f1bca02",
   "metadata": {},
   "outputs": [],
   "source": [
    "sr = 22050\n",
    "wsize, hsize, mel_bins = 520, 320, 128\n",
    "fmin, fmax = 50, 8000\n",
    "fcs, dropout, act = [2048], 0.5, nn.ReLU\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model = WaveNet(1, 2048, sr=sr, wsize=wsize, hsize=hsize, mel_bins=mel_bins, \n",
    "                fmin=fmin, fmax=fmax, fcs=fcs, dropout=dropout, act=act).half().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "53c6d159",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "data_dir = r\"D:\\Documents\\datasets\\AIST4010\\muse\"\n",
    "wav_dir = os.path.join(data_dir, \"wavs\")\n",
    "songs_fp = os.path.join(data_dir, \"extracted_data.csv\")\n",
    "\n",
    "def get_labels(ids, fp=songs_fp):\n",
    "    songs_data = pd.read_csv(fp)\n",
    "    songs_data.set_index(\"spotify_id\", inplace=True)\n",
    "    labels = songs_data.loc[ids, [\"valence_tags\", \"arousal_tags\", \"dominance_tags\"]].values\n",
    "    return labels\n",
    "\n",
    "class LazyWavDataset(Dataset):\n",
    "    def __init__(self, dir_path, labels, re_pattern, \n",
    "                 transform=None, extension='.npy', \n",
    "                 pad_len=661500, sample_size=8500):\n",
    "        super().__init__()\n",
    "        self.files = glob.glob(os.path.join(dir_path, '*'+extension))\n",
    "        self.files.sort(key=lambda fp: re.match(re_pattern, fp).group(1))\n",
    "        self.ids = [re.match(re_pattern, fp).group(1) for fp in self.files]\n",
    "        self.labels = torch.from_numpy(labels)\n",
    "        if sample_size:\n",
    "            self.files = self.files[:8500]\n",
    "            self.labels = self.labels[:8500]\n",
    "        self.transform = transform\n",
    "        self.pad_len = pad_len\n",
    "    \n",
    "    def __getitem__(self, item):\n",
    "        file = self.files[item]\n",
    "        song_id = self.ids[item]\n",
    "        wav = np.load(file)\n",
    "        pad_len = self.pad_len\n",
    "        if pad_len:\n",
    "            if len(wav) >= pad_len:\n",
    "                wav = wav[:pad_len]\n",
    "            else:\n",
    "                wav = np.pad(wav, (0, pad_len - len(wav)), \n",
    "                             mode='constant', constant_values=(0, 0))\n",
    "        file = torch.from_numpy(wav)\n",
    "        label = self.labels[item]\n",
    "        if self.transform:\n",
    "            file = self.transform(file)\n",
    "        return song_id, file, label\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b4d7bf18",
   "metadata": {},
   "outputs": [],
   "source": [
    "npy_fps = glob.glob(os.path.join(wav_dir, '*.npy'))\n",
    "re_pattern = r\".*\\\\([^\\\\\\.]*)\\.npy\"\n",
    "npy_fps.sort(key=lambda fp: re.match(re_pattern, fp).group(1))\n",
    "song_ids = [re.match(re_pattern, fp).group(1) for fp in npy_fps]\n",
    "labels = get_labels(song_ids)[:, 0] / 9\n",
    "\n",
    "train_size = 8500\n",
    "ds = LazyWavDataset(wav_dir, labels, re_pattern, sample_size=train_size)\n",
    "loader = DataLoader(ds, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dbbe76dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.17147\n",
      "loss: 0.06327\n",
      "loss: 0.04927\n",
      "loss: 0.04261\n",
      "loss: 0.04078\n",
      "loss: 0.03924\n",
      "loss: 0.03784\n",
      "loss: 0.03665\n",
      "loss: 0.03703\n",
      "loss: 0.03545\n"
     ]
    }
   ],
   "source": [
    "LR, MOMENTUM, DECAY = 1e-3, 0.9, 1e-3\n",
    "criterion = nn.MSELoss()\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=LR, weight_decay=DECAY)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=LR, \n",
    "                            momentum=MOMENTUM, weight_decay=DECAY)\n",
    "\n",
    "EPOCHS = 10\n",
    "for epoch in range(EPOCHS):\n",
    "    running_loss = 0\n",
    "    for ids, wavs, labels in loader:\n",
    "        optimizer.zero_grad()\n",
    "        wavs, labels = wavs.half().to(device), labels.reshape(-1, 1).half().to(device)\n",
    "#         input()\n",
    "        outputs = model(wavs, None)\n",
    "#         input()\n",
    "#         print(outputs.shape, wavs.shape, labels.shape)\n",
    "#         input()\n",
    "#         break\n",
    "        loss = criterion(outputs, labels)\n",
    "        running_loss += loss.item() * len(ids)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    epoch_loss = running_loss / len(loader.dataset)\n",
    "    print(f\"loss: {epoch_loss:.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d918f61c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(33.6250, device='cuda:0', dtype=torch.float16,\n",
       "       grad_fn=<MseLossBackward0>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "criterion(outputs, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5061fcfd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21168056"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.getsizeof(wavs.storage())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5c2b991a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48d04b57",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "music-analysis",
   "language": "python",
   "name": "music-analysis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
